# ===========================================
# LLM (Large Language Model) Configuration
# ===========================================
# This file configures the LLM client protection mechanisms and model parameters.
# Copy this file to .env.llm and adjust values based on your needs.

# ===========================================
# Circuit Breaker Configuration
# ===========================================
# Circuit breaker prevents cascading failures by stopping requests to failing services.
# It has three states: CLOSED (normal), OPEN (blocking), HALF_OPEN (testing recovery).

# Time in milliseconds before circuit breaker resets from 'open' to 'half-open'
# After this timeout, the circuit breaker will allow a test request through
# Default: 20000 (20 seconds), Range: 1000-300000 (1s to 5min)
DEFAULT_CIRCUIT_BREAKER_RESET_TIMEOUT=20000

# Request timeout in milliseconds
# If a request takes longer than this, it's considered failed
# Default: 30000 (30 seconds), Range: 1000-300000 (1s to 5min)
DEFAULT_CIRCUIT_BREAKER_TIMEOUT=30000

# Error threshold percentage (0-100) to trip the circuit breaker
# If error rate exceeds this percentage, circuit opens
# Default: 50 (50%), Range: 1-100
# Example: 50 means if 50% of requests fail, circuit opens
DEFAULT_CIRCUIT_BREAKER_ERROR_THRESHOLD=50

# Rolling window duration in milliseconds for calculating error rate
# Statistics are calculated over this time window
# Default: 10000 (10 seconds), Range: 1000-60000 (1s to 1min)
DEFAULT_CIRCUIT_BREAKER_ROLLING_COUNT_TIMEOUT=10000

# Minimum number of requests before circuit breaker can trip
# Circuit won't open until at least this many requests have been made
# Default: 10, Range: 1-1000
# Example: Circuit needs at least 10 requests before it can open
DEFAULT_CIRCUIT_BREAKER_VOLUME_THRESHOLD=10

# Maximum number of concurrent requests
# Limits the total number of simultaneous requests
# Default: 100, Range: 1-10000
DEFAULT_CIRCUIT_BREAKER_CAPACITY=100

# Circuit breaker name (optional)
# Used for logging and monitoring multiple circuit breakers
# DEFAULT_CIRCUIT_BREAKER_NAME=main-breaker

# ===========================================
# Rate Limiter Configuration
# ===========================================
# Rate limiter controls the number of requests per time period.
# Prevents overwhelming the LLM API and helps manage costs.

# Maximum requests per minute
# Limits API calls to stay within provider's rate limits and control costs
# Default: 60, Range: 1-10000
# Note: Check your LLM provider's rate limits (e.g., DeepSeek, OpenAI)
DEFAULT_RATE_LIMITER_MAX_REQUESTS_PER_MINUTE=60

# ===========================================
# Request Queue Configuration
# ===========================================
# Request queue manages concurrent request execution.
# Queues requests when concurrent limit is reached.

# Maximum concurrent requests
# Number of requests that can run simultaneously
# Default: 5, Range: 1-1000
# Lower values: Better stability, slower throughput
# Higher values: Faster throughput, more resource usage
DEFAULT_REQUEST_QUEUE_MAX_CONCURRENT=5

# ===========================================
# Retry Configuration
# ===========================================
# Retry mechanism handles transient failures with exponential backoff.
# Automatically retries failed requests with increasing delays.

# Maximum number of retry attempts
# Total retry attempts after initial failure
# Default: 3, Range: 0-10
# Set to 0 to disable retries
DEFAULT_RETRY_MAX_RETRIES=3

# Initial delay between retries in milliseconds
# Starting delay before first retry
# Default: 1000 (1 second), Range: 100-60000 (0.1s to 60s)
DEFAULT_RETRY_INITIAL_DELAY_MS=1000

# Maximum delay between retries in milliseconds
# Upper limit for exponential backoff
# Default: 30000 (30 seconds), Range: 1000-300000 (1s to 5min)
DEFAULT_RETRY_MAX_DELAY_MS=30000

# Exponential backoff factor
# Multiplier for delay calculation: delay = initialDelay * (factor ^ retryCount)
# Default: 2, Range: 1.1-10
# Example: With factor=2, delays are 1s, 2s, 4s, 8s...
DEFAULT_RETRY_FACTOR=2

# Enable jitter (random variation) in retry delays
# Adds randomness to prevent thundering herd problem
# Default: true, Values: true/false or 1/0
# Recommended: true for distributed systems
DEFAULT_RETRY_JITTER=true

# Custom retryable error codes (comma-separated regex patterns)
# Specify which error codes should trigger retries
# Example: 502,504,ECONNREFUSED,ETIMEDOUT
# RETRYABLE_ERROR_CODES=502,504,ECONNREFUSED

# ===========================================
# Model Configuration
# ===========================================
# Large Language Model parameters that control generation behavior.

# Default model name
# Model identifier from your LLM provider
# Examples: 'deepseek-chat', 'gpt-4', 'gpt-3.5-turbo'
DEFAULT_MODEL_NAME=deepseek-chat

# Temperature (0-2): Controls randomness in generation
# 0 = Deterministic, focused responses
# 1 = Balanced creativity and coherence
# 2 = Maximum creativity and randomness
# Default: 0, Range: 0-2
# Recommended: 0-0.3 for factual tasks, 0.7-1.0 for creative tasks
DEFAULT_MODEL_TEMPERATURE=0

# Top P (0-1): Nucleus sampling parameter
# Controls diversity via cumulative probability
# 1 = Consider all tokens (maximum diversity)
# 0.1 = Only consider top 10% probable tokens
# Default: 1, Range: 0-1
# Tip: Use either temperature OR top_p, not both
DEFAULT_MODEL_TOP_P=1

# Frequency Penalty (-2.0 to 2.0): Reduces token repetition
# Penalizes tokens based on their frequency in the text so far
# Positive values: Reduce repetition
# Negative values: Encourage repetition
# Default: 0, Range: -2.0 to 2.0
DEFAULT_MODEL_FREQUENCY_PENALTY=0

# Presence Penalty (-2.0 to 2.0): Encourages topic diversity
# Penalizes tokens that have appeared in the text
# Positive values: Encourage new topics
# Negative values: Focus on existing topics
# Default: 0, Range: -2.0 to 2.0
DEFAULT_MODEL_PRESENCE_PENALTY=0

# Max Tokens: Maximum tokens to generate
# Controls the length of the response
# Note: 1 token â‰ˆ 4 characters for English text
# Default: 4096, Range: 1-model's maximum (varies by model)
# Warning: Higher values = higher API costs
DEFAULT_MODEL_MAX_TOKENS=4096

# Max Concurrency: Maximum concurrent model requests
# Number of parallel API calls to the LLM provider
# Default: 10, Range: 1-100
# Note: Must comply with provider's rate limits
DEFAULT_MODEL_MAX_CONCURRENCY=10

# Max Retries: Maximum retry attempts for model requests
# Specific retries for model API failures
# Default: 3, Range: 0-10
DEFAULT_MODEL_MAX_RETRIES=3

# Timeout (ms): Request timeout
# Maximum time to wait for model response
# Default: 60000 (60 seconds), Range: 1000-300000 (1s to 5min)
# Note: Longer generation (high max_tokens) needs longer timeout
DEFAULT_MODEL_TIMEOUT=60000

# ===========================================
# Workflow Configuration
# ===========================================
# Settings for LangGraph workflow execution.

# Workflow timeout in milliseconds
# Maximum time for entire workflow execution
# Default: 600000 (10 minutes), Range: 60000-3600000 (1min to 1hour)
# Note: Complex workflows may need longer timeouts
DEFAULT_WORKFLOW_TIMEOUT=600000

# Maximum recursion depth for workflow execution
# Prevents infinite loops in graph execution
# Default: 50, Range: 10-1000
# Higher values allow more complex workflows but risk stack overflow
DEFAULT_RECURSION_LIMIT=50

